{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import requests as rq\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Web address for each status category\n",
    "active_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Active'\n",
    "a = rq.get(active_url)\n",
    "a_soup = bsoup(a.text,\"lxml\")\n",
    "\n",
    "deceased_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Deceased'\n",
    "d = rq.get(deceased_url)\n",
    "d_soup = bsoup(d.text,\"lxml\")\n",
    "\n",
    "disability_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disability+Inactive'\n",
    "di = rq.get(disability_url)\n",
    "di_soup = bsoup(di.text,\"lxml\")\n",
    "\n",
    "disbarred_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disbarred'\n",
    "db = rq.get(disbarred_url)\n",
    "db_soup = bsoup(db.text,\"lxml\")\n",
    "\n",
    "emeritus_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Emeritus'\n",
    "e = rq.get(emeritus_url)\n",
    "e_soup = bsoup(e.text,\"lxml\")\n",
    "\n",
    "foreign_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Foreign+Law+Consultant'\n",
    "f = rq.get(foreign_url)\n",
    "f_soup = bsoup(f.text,\"lxml\")\n",
    "\n",
    "honorary_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Honorary'\n",
    "h = rq.get(honorary_url)\n",
    "h_soup = bsoup(h.text,\"lxml\")\n",
    "\n",
    "house_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=House+Counsel'\n",
    "hc = rq.get(house_url)\n",
    "hc_soup = bsoup(hc.text,\"lxml\")\n",
    "\n",
    "inactive_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Inactive+Attorney'\n",
    "i = rq.get(inactive_url)\n",
    "i_soup = bsoup(i.text,\"lxml\")\n",
    "\n",
    "judicial_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Judicial'\n",
    "j = rq.get(judicial_url)\n",
    "j_soup = bsoup(j.text,\"lxml\")\n",
    "\n",
    "military_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Military'\n",
    "m = rq.get(military_url)\n",
    "m_soup = bsoup(m.text,\"lxml\")\n",
    "\n",
    "rinlieu_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Resigned+in+Lieu...'\n",
    "ril = rq.get(rinlieu_url)\n",
    "ril_soup = bsoup(ril.text,\"lxml\")\n",
    "\n",
    "suspended_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Suspended'\n",
    "s = rq.get(suspended_url)\n",
    "s_soup = bsoup(s.text,\"lxml\")\n",
    "\n",
    "terminated_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Terminated'\n",
    "t = rq.get(terminated_url)\n",
    "t_soup = bsoup(t.text,\"lxml\")\n",
    "\n",
    "resigned_url = 'https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Voluntarily+Resigned'\n",
    "r = rq.get(resigned_url)\n",
    "r_soup = bsoup(r.text,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Total number of pages for each status category. Set at 2 pages for demonstration purposes.\n",
    "a_num_pages = 2\n",
    "d_num_pages = 2\n",
    "di_num_pages = 2\n",
    "db_num_pages = 2\n",
    "e_num_pages = 2\n",
    "f_num_pages = 2\n",
    "h_num_pages = 2\n",
    "hc_num_pages = 2\n",
    "i_num_pages = 2\n",
    "j_num_pages = 2\n",
    "# m_num_pages = 2_ Military only has one page\n",
    "ril_num_pages = 2\n",
    "s_num_pages = 2\n",
    "t_num_pages = 2\n",
    "r_num_pages = 2\n",
    "\n",
    "# Format URL to reflect the first page of results (primary) for each status category and subsequent pages (secondary)\n",
    "active_primary = [\"{}\".format(active_url)]\n",
    "active_secondary = [\"{}&Page={}\".format(active_url, str(page)) for page in range(1,a_num_pages +1)]\n",
    "\n",
    "deceased_primary = [\"{}\".format(deceased_url)]\n",
    "deceased_secondary = [\"{}&Page={}\".format(deceased_url, str(page)) for page in range(1,d_num_pages +1)]\n",
    "\n",
    "disability_primary = [\"{}\".format(disability_url)]\n",
    "disability_secondary = [\"{}&Page={}\".format(disability_url, str(page)) for page in range(1,di_num_pages +1)]\n",
    "\n",
    "disbarred_primary = [\"{}\".format(disbarred_url)]\n",
    "disbarred_secondary = [\"{}&Page={}\".format(disbarred_url, str(page)) for page in range(1,db_num_pages +1)]\n",
    "\n",
    "emeritus_primary = [\"{}\".format(emeritus_url)]\n",
    "emeritus_secondary = [\"{}&Page={}\".format(emeritus_url, str(page)) for page in range(1,e_num_pages +1)]\n",
    "\n",
    "foreign_primary = [\"{}\".format(foreign_url)]\n",
    "foreign_secondary = [\"{}&Page={}\".format(foreign_url, str(page)) for page in range(1,h_num_pages +1)]\n",
    "\n",
    "honorary_primary = [\"{}\".format(honorary_url)]\n",
    "honorary_secondary = [\"{}&Page={}\".format(honorary_url, str(page)) for page in range(1,h_num_pages +1)]\n",
    "\n",
    "house_primary = [\"{}\".format(house_url)]\n",
    "house_secondary = [\"{}&Page={}\".format(house_url, str(page)) for page in range(1,hc_num_pages +1)]\n",
    "\n",
    "inactive_primary = [\"{}\".format(inactive_url)]\n",
    "inactive_secondary = [\"{}&Page={}\".format(inactive_url, str(page)) for page in range(1,i_num_pages +1)]\n",
    "\n",
    "judicial_primary = [\"{}\".format(judicial_url)]\n",
    "judicial_secondary = [\"{}&Page={}\".format(judicial_url, str(page)) for page in range(1,j_num_pages +1)]\n",
    "\n",
    "military_primary = [\"{}\".format(military_url)]\n",
    "# military_secondary = [\"{}&Page={}\".format(military_url, str(page)) for page in range(1,m_num_pages +1)]\n",
    "\n",
    "rinlieu_primary = [\"{}\".format(rinlieu_url)]\n",
    "rinlieu_secondary = [\"{}&Page={}\".format(rinlieu_url, str(page)) for page in range(1,ril_num_pages +1)]\n",
    "\n",
    "suspended_primary = [\"{}\".format(suspended_url)]\n",
    "suspended_secondary = [\"{}&Page={}\".format(suspended_url, str(page)) for page in range(1,s_num_pages +1)]\n",
    "\n",
    "terminated_primary = [\"{}\".format(terminated_url)]\n",
    "terminated_secondary = [\"{}&Page={}\".format(terminated_url, str(page)) for page in range(1,t_num_pages +1)]\n",
    "\n",
    "resigned_primary = [\"{}\".format(resigned_url)]\n",
    "resigned_secondary = [\"{}&Page={}\".format(resigned_url, str(page)) for page in range(1,r_num_pages +1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Active...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Active...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Active&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Active&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Active&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Active&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Deceased...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Deceased...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Deceased&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Deceased&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Deceased&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Deceased&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disability+Inactive...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disability+Inactive...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disability+Inactive&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disability+Inactive&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disability+Inactive&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disability+Inactive&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disbarred...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disbarred...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disbarred&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disbarred&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disbarred&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Disbarred&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Emeritus...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Emeritus...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Emeritus&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Emeritus&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Emeritus&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Emeritus&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Foreign+Law+Consultant...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Foreign+Law+Consultant...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Foreign+Law+Consultant&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Foreign+Law+Consultant&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Foreign+Law+Consultant&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Foreign+Law+Consultant&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Honorary...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Honorary...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Honorary&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Honorary&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Honorary&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Honorary&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=House+Counsel...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=House+Counsel...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=House+Counsel&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=House+Counsel&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=House+Counsel&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=House+Counsel&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Inactive+Attorney...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Inactive+Attorney...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Inactive+Attorney&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Inactive+Attorney&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Inactive+Attorney&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Inactive+Attorney&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Judicial...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Judicial...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Judicial&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Judicial&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Judicial&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Judicial&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Military...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Military...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Resigned+in+Lieu......\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Resigned+in+Lieu......\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Resigned+in+Lieu...&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Resigned+in+Lieu...&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Resigned+in+Lieu...&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Resigned+in+Lieu...&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Suspended...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Suspended...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Suspended&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Suspended&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Suspended&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Suspended&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Terminated...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Terminated...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Terminated&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Terminated&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Terminated&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Terminated&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Voluntarily+Resigned...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Voluntarily+Resigned...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Voluntarily+Resigned&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Voluntarily+Resigned&Page=2...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Voluntarily+Resigned&Page=1...\n",
      "Processing https://www.mywsba.org/Default.aspx?tabid=191&ShowSearchResults=TRUE&Status=Voluntarily+Resigned&Page=2...\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Scrape the site and save output to a csv file. Four for loops were necessary for each category: one each to capture the two \n",
    "URL formats and one each to capture the two CSS styles in the table \"\"\"\n",
    "\n",
    "with open(\"results.csv\",\"wb\") as acct:\n",
    "    for url in active_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        a_new = rq.get(url)\n",
    "        soup_new = bsoup(a_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "\n",
    "    for url in active_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        a_new = rq.get(url)\n",
    "        soup_new = bsoup(a_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in active_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        a_new = rq.get(url)\n",
    "        soup_new = bsoup(a_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "\n",
    "    for url in active_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        a_new = rq.get(url)\n",
    "        soup_new = bsoup(a_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in deceased_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        d_new = rq.get(url)\n",
    "        soup_new = bsoup(d_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in deceased_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        d_new = rq.get(url)\n",
    "        soup_new = bsoup(d_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "                        \n",
    "    for url in deceased_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        d_new = rq.get(url)\n",
    "        soup_new = bsoup(d_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "          \n",
    "    for url in deceased_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        d_new = rq.get(url)\n",
    "        soup_new = bsoup(d_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in disability_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        di_new = rq.get(url)\n",
    "        soup_new = bsoup(di_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in disability_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        di_new = rq.get(url)\n",
    "        soup_new = bsoup(di_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in disability_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        di_new = rq.get(url)\n",
    "        soup_new = bsoup(di_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "        \n",
    "    for url in disability_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        di_new = rq.get(url)\n",
    "        soup_new = bsoup(di_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in disbarred_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        db_new = rq.get(url)\n",
    "        soup_new = bsoup(db_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in disbarred_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        db_new = rq.get(url)\n",
    "        soup_new = bsoup(db_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in disbarred_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        db_new = rq.get(url)\n",
    "        soup_new = bsoup(db_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in disbarred_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        db_new = rq.get(url)\n",
    "        soup_new = bsoup(db_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in emeritus_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        e_new = rq.get(url)\n",
    "        soup_new = bsoup(e_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "  \n",
    "    for url in emeritus_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        e_new = rq.get(url)\n",
    "        soup_new = bsoup(e_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in emeritus_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        e_new = rq.get(url)\n",
    "        soup_new = bsoup(e_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in emeritus_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        e_new = rq.get(url)\n",
    "        soup_new = bsoup(e_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in foreign_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        f_new = rq.get(url)\n",
    "        soup_new = bsoup(f_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "   \n",
    "    for url in foreign_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        f_new = rq.get(url)\n",
    "        soup_new = bsoup(f_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in foreign_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        f_new = rq.get(url)\n",
    "        soup_new = bsoup(f_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    " \n",
    "    for url in foreign_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        f_new = rq.get(url)\n",
    "        soup_new = bsoup(f_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in honorary_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        h_new = rq.get(url)\n",
    "        soup_new = bsoup(h_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in honorary_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        h_new = rq.get(url)\n",
    "        soup_new = bsoup(h_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in honorary_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        h_new = rq.get(url)\n",
    "        soup_new = bsoup(h_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "         \n",
    "    for url in honorary_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        h_new = rq.get(url)\n",
    "        soup_new = bsoup(h_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in house_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        hc_new = rq.get(url)\n",
    "        soup_new = bsoup(hc_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in house_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        hc_new = rq.get(url)\n",
    "        soup_new = bsoup(hc_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in house_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        hc_new = rq.get(url)\n",
    "        soup_new = bsoup(hc_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in house_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        hc_new = rq.get(url)\n",
    "        soup_new = bsoup(hc_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in inactive_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        i_new = rq.get(url)\n",
    "        soup_new = bsoup(i_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in inactive_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        i_new = rq.get(url)\n",
    "        soup_new = bsoup(i_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in inactive_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        i_new = rq.get(url)\n",
    "        soup_new = bsoup(i_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "\n",
    "    for url in inactive_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        i_new = rq.get(url)\n",
    "        soup_new = bsoup(i_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in judicial_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        j_new = rq.get(url)\n",
    "        soup_new = bsoup(j_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in judicial_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        j_new = rq.get(url)\n",
    "        soup_new = bsoup(j_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in judicial_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        j_new = rq.get(url)\n",
    "        soup_new = bsoup(j_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "      \n",
    "    for url in judicial_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        j_new = rq.get(url)\n",
    "        soup_new = bsoup(j_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in military_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        m_new = rq.get(url)\n",
    "        soup_new = bsoup(m_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in military_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        m_new = rq.get(url)\n",
    "        soup_new = bsoup(m_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    \"\"\"for url in military_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        m_new = rq.get(url)\n",
    "        soup_new = bsoup(m_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in military_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        m_new = rq.get(url)\n",
    "        soup_new = bsoup(m_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\"\"\"\n",
    "    \n",
    "    for url in rinlieu_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        ril_new = rq.get(url)\n",
    "        soup_new = bsoup(ril_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in rinlieu_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        ril_new = rq.get(url)\n",
    "        soup_new = bsoup(ril_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in rinlieu_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        ril_new = rq.get(url)\n",
    "        soup_new = bsoup(ril_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in rinlieu_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        ril_new = rq.get(url)\n",
    "        soup_new = bsoup(ril_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in suspended_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        s_new = rq.get(url)\n",
    "        soup_new = bsoup(s_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in suspended_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        s_new = rq.get(url)\n",
    "        soup_new = bsoup(s_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in suspended_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        s_new = rq.get(url)\n",
    "        soup_new = bsoup(s_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in suspended_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        s_new = rq.get(url)\n",
    "        soup_new = bsoup(s_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in terminated_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        t_new = rq.get(url)\n",
    "        soup_new = bsoup(t_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in terminated_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        t_new = rq.get(url)\n",
    "        soup_new = bsoup(t_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in terminated_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        t_new = rq.get(url)\n",
    "        soup_new = bsoup(t_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in terminated_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        t_new = rq.get(url)\n",
    "        soup_new = bsoup(t_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in resigned_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        r_new = rq.get(url)\n",
    "        soup_new = bsoup(r_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "    \n",
    "    for url in resigned_primary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        r_new = rq.get(url)\n",
    "        soup_new = bsoup(r_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):\n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "            \n",
    "    for url in resigned_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        r_new = rq.get(url)\n",
    "        soup_new = bsoup(r_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')\n",
    "\n",
    "    for url in resigned_secondary:\n",
    "        print \"Processing {}...\".format(url)\n",
    "        r_new = rq.get(url)\n",
    "        soup_new = bsoup(r_new.text, \"lxml\")\n",
    "        for tr in soup_new.find_all('tr',{'class':'itemAlternateStyle'}):                            \n",
    "            stack = []\n",
    "            for td in tr.findAll('td'):\n",
    "                stack.append(td.text.replace('\\n', '').strip())\n",
    "            acct.write(', '.join(stack).encode('utf-8') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
